{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Creating Redshift Cluster using the AWS python SDK \n",
    "## An example of Infrastructure-as-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load parameters from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                    Param       Value\n0        DWH_CLUSTER_TYPE  multi-node\n1           DWH_NUM_NODES           4\n2           DWH_NODE_TYPE   dc2.large\n3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n4                  DWH_DB    sparkify\n5             DWH_DB_USER       spark\n6         DWH_DB_PASSWORD  Sparkpass0\n7                DWH_PORT        5439\n8       DWH_IAM_ROLE_NAME     dwhRole",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Param</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DWH_CLUSTER_TYPE</td>\n      <td>multi-node</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DWH_NUM_NODES</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DWH_NODE_TYPE</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DWH_CLUSTER_IDENTIFIER</td>\n      <td>dwhCluster</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DWH_DB</td>\n      <td>sparkify</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>DWH_DB_USER</td>\n      <td>spark</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>DWH_DB_PASSWORD</td>\n      <td>Sparkpass0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>DWH_PORT</td>\n      <td>5439</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>DWH_IAM_ROLE_NAME</td>\n      <td>dwhRole</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "PERSONAL_IP            = config.get('NETWORK','ELASTIC_IP')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"CLUSTER\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"CLUSTER\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"CLUSTER\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"CLUSTER\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"CLUSTER\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"CLUSTER\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"CLUSTER\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"CLUSTER\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"CLUSTER\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create clients for IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "\n",
    "ec2 = boto3.client('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check song objects from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs keys:  14897\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "kwargs = dict(Bucket='udacity-dend',Prefix='song_data')\n",
    "\n",
    "num_songs=0\n",
    "while True:\n",
    "    get_folder_objects = s3.list_objects_v2(\n",
    "           **kwargs\n",
    "          )\n",
    "\n",
    "    num_songs+=get_folder_objects[\"KeyCount\"]\n",
    "    if not 'NextContinuationToken' in get_folder_objects :\n",
    "        break\n",
    "    else:\n",
    "        kwargs = dict(Bucket='udacity-dend',Prefix='song_data',ContinuationToken=get_folder_objects['NextContinuationToken'])\n",
    "\n",
    "print(\"Number of songs keys: \",num_songs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# STEP 1: IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::703838047664:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STEP 2: Open an incoming  TCP port to access the cluster endpoint\n",
    "(That will allow us to connect to the cluster )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security Group Created sg-025041d3c35ff3d2f in vpc vpc-6f583517.\n",
      "Ingress Successfully Set {'ResponseMetadata': {'RequestId': 'e0d0ad75-7888-42cd-8345-93679cbce5a5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e0d0ad75-7888-42cd-8345-93679cbce5a5', 'content-type': 'text/xml;charset=UTF-8', 'content-length': '259', 'date': 'Thu, 23 Apr 2020 09:48:41 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    default_vpc_id = ec2.describe_vpcs()['Vpcs'][0]['VpcId']\n",
    "\n",
    "    response = ec2.create_security_group(GroupName='redshift_ingress_sg',\n",
    "                                         Description='Allow tcp from the defined elastic ip',\n",
    "                                         VpcId=default_vpc_id)\n",
    "    security_group_id = response['GroupId']\n",
    "    print('Security Group Created %s in vpc %s.' % (security_group_id, default_vpc_id))\n",
    "\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=security_group_id,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': int(DWH_PORT),\n",
    "             'ToPort': int(DWH_PORT),\n",
    "             'IpRanges': [{'CidrIp': PERSONAL_IP}]}\n",
    "        ])\n",
    "    print('Ingress Successfully Set %s' % data)\n",
    "except ClientError as e:\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3:  Redshift Cluster\n",
    "\n",
    " Create a RedShift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        VpcSecurityGroupIds=[security_group_id],\n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 3.1 *Describe* the cluster to see its status\n",
    "- run this block several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                 Key  \\\n0  ClusterIdentifier   \n1           NodeType   \n2      ClusterStatus   \n3     MasterUsername   \n4             DBName   \n5           Endpoint   \n6              VpcId   \n7      NumberOfNodes   \n\n                                                                                   Value  \n0                                                                             dwhcluster  \n1                                                                              dc2.large  \n2                                                                              available  \n3                                                                                  spark  \n4                                                                               sparkify  \n5  {'Address': 'dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n6                                                                           vpc-6f583517  \n7                                                                                      4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ClusterIdentifier</td>\n      <td>dwhcluster</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NodeType</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClusterStatus</td>\n      <td>available</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MasterUsername</td>\n      <td>spark</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBName</td>\n      <td>sparkify</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Endpoint</td>\n      <td>{'Address': 'dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>VpcId</td>\n      <td>vpc-6f583517</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NumberOfNodes</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth',None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get cluster endpoint and relevant role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::703838047664:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Make sure you can connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://spark:Sparkpass0@dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com:5439/sparkify\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Connected: spark@sparkify'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Run the create_tables.py & etl.py to create and stage the data\n",
    "(add to the HOST the endpoint value from step 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished load query : 1\r\n",
      "Finished load query : 2\r\n",
      "Finished insert query : 1\r\n",
      "Finished insert query : 2\r\n",
      "Finished insert query : 3\r\n",
      "Finished insert query : 4\r\n",
      "Finished insert query : 5\r\n"
     ]
    }
   ],
   "source": [
    "!python create_tables.py\n",
    "!python etl.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# STEP 6: Run an analytics query\n",
    "##Give the top 20 users by unique songs listened to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://spark:***@dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com:5439/sparkify\n",
      "20 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[(49, 'Chloe', 'Cuevas', 34),\n (80, 'Tegan', 'Levine', 30),\n (97, 'Kate', 'Harrell', 27),\n (44, 'Aleena', 'Kirby', 21),\n (88, 'Mohammad', 'Rodriguez', 16),\n (73, 'Jacob', 'Klein', 15),\n (29, 'Jacqueline', 'Lynch', 13),\n (15, 'Lily', 'Koch', 13),\n (36, 'Matthew', 'Jones', 12),\n (24, 'Layla', 'Griffin', 11),\n (58, 'Emily', 'Benson', 9),\n (25, 'Jayden', 'Graves', 9),\n (30, 'Avery', 'Watkins', 8),\n (95, 'Sara', 'Johnson', 8),\n (85, 'Kinsley', 'Young', 7),\n (42, 'Harper', 'Barrett', 6),\n (16, 'Rylan', 'George', 5),\n (50, 'Ava', 'Robinson', 4),\n (8, 'Kaylee', 'Summers', 3),\n (82, 'Avery', 'Martinez', 3)]",
      "text/html": "<table>\n    <tr>\n        <th>user_id</th>\n        <th>first_name</th>\n        <th>last_name</th>\n        <th>songs_for_user</th>\n    </tr>\n    <tr>\n        <td>49</td>\n        <td>Chloe</td>\n        <td>Cuevas</td>\n        <td>34</td>\n    </tr>\n    <tr>\n        <td>80</td>\n        <td>Tegan</td>\n        <td>Levine</td>\n        <td>30</td>\n    </tr>\n    <tr>\n        <td>97</td>\n        <td>Kate</td>\n        <td>Harrell</td>\n        <td>27</td>\n    </tr>\n    <tr>\n        <td>44</td>\n        <td>Aleena</td>\n        <td>Kirby</td>\n        <td>21</td>\n    </tr>\n    <tr>\n        <td>88</td>\n        <td>Mohammad</td>\n        <td>Rodriguez</td>\n        <td>16</td>\n    </tr>\n    <tr>\n        <td>73</td>\n        <td>Jacob</td>\n        <td>Klein</td>\n        <td>15</td>\n    </tr>\n    <tr>\n        <td>29</td>\n        <td>Jacqueline</td>\n        <td>Lynch</td>\n        <td>13</td>\n    </tr>\n    <tr>\n        <td>15</td>\n        <td>Lily</td>\n        <td>Koch</td>\n        <td>13</td>\n    </tr>\n    <tr>\n        <td>36</td>\n        <td>Matthew</td>\n        <td>Jones</td>\n        <td>12</td>\n    </tr>\n    <tr>\n        <td>24</td>\n        <td>Layla</td>\n        <td>Griffin</td>\n        <td>11</td>\n    </tr>\n    <tr>\n        <td>58</td>\n        <td>Emily</td>\n        <td>Benson</td>\n        <td>9</td>\n    </tr>\n    <tr>\n        <td>25</td>\n        <td>Jayden</td>\n        <td>Graves</td>\n        <td>9</td>\n    </tr>\n    <tr>\n        <td>30</td>\n        <td>Avery</td>\n        <td>Watkins</td>\n        <td>8</td>\n    </tr>\n    <tr>\n        <td>95</td>\n        <td>Sara</td>\n        <td>Johnson</td>\n        <td>8</td>\n    </tr>\n    <tr>\n        <td>85</td>\n        <td>Kinsley</td>\n        <td>Young</td>\n        <td>7</td>\n    </tr>\n    <tr>\n        <td>42</td>\n        <td>Harper</td>\n        <td>Barrett</td>\n        <td>6</td>\n    </tr>\n    <tr>\n        <td>16</td>\n        <td>Rylan</td>\n        <td>George</td>\n        <td>5</td>\n    </tr>\n    <tr>\n        <td>50</td>\n        <td>Ava</td>\n        <td>Robinson</td>\n        <td>4</td>\n    </tr>\n    <tr>\n        <td>8</td>\n        <td>Kaylee</td>\n        <td>Summers</td>\n        <td>3</td>\n    </tr>\n    <tr>\n        <td>82</td>\n        <td>Avery</td>\n        <td>Martinez</td>\n        <td>3</td>\n    </tr>\n</table>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select s.user_id,u.first_name,u.last_name, COUNT(DISTINCT s.song_id) as songs_for_user from songplays s\n",
    "NATURAL JOIN users u\n",
    "group by s.user_id,u.first_name,u.last_name order by songs_for_user desc limit 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n  'NodeType': 'dc2.large',\n  'ClusterStatus': 'deleting',\n  'ClusterAvailabilityStatus': 'Modifying',\n  'MasterUsername': 'spark',\n  'DBName': 'sparkify',\n  'Endpoint': {'Address': 'dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com',\n   'Port': 5439},\n  'ClusterCreateTime': datetime.datetime(2020, 4, 23, 9, 53, 42, 608000, tzinfo=tzutc()),\n  'AutomatedSnapshotRetentionPeriod': 1,\n  'ManualSnapshotRetentionPeriod': -1,\n  'ClusterSecurityGroups': [],\n  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-025041d3c35ff3d2f',\n    'Status': 'active'}],\n  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n    'ParameterApplyStatus': 'in-sync'}],\n  'ClusterSubnetGroupName': 'default',\n  'VpcId': 'vpc-6f583517',\n  'AvailabilityZone': 'us-west-2c',\n  'PreferredMaintenanceWindow': 'tue:10:30-tue:11:00',\n  'PendingModifiedValues': {},\n  'ClusterVersion': '1.0',\n  'AllowVersionUpgrade': True,\n  'NumberOfNodes': 4,\n  'PubliclyAccessible': True,\n  'Encrypted': False,\n  'Tags': [],\n  'EnhancedVpcRouting': False,\n  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::703838047664:role/dwhRole',\n    'ApplyStatus': 'in-sync'}],\n  'MaintenanceTrackName': 'current',\n  'DeferredMaintenanceWindows': [],\n  'NextMaintenanceWindowStartTime': datetime.datetime(2020, 4, 28, 10, 30, tzinfo=tzutc())},\n 'ResponseMetadata': {'RequestId': '8b5e3716-3eae-4d82-8eb7-025e03e13ac4',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '8b5e3716-3eae-4d82-8eb7-025e03e13ac4',\n   'content-type': 'text/xml',\n   'content-length': '2394',\n   'vary': 'accept-encoding',\n   'date': 'Thu, 23 Apr 2020 10:24:18 GMT'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                 Key  \\\n0  ClusterIdentifier   \n1           NodeType   \n2      ClusterStatus   \n3     MasterUsername   \n4             DBName   \n5           Endpoint   \n6              VpcId   \n7      NumberOfNodes   \n\n                                                                                   Value  \n0                                                                             dwhcluster  \n1                                                                              dc2.large  \n2                                                                               deleting  \n3                                                                                  spark  \n4                                                                               sparkify  \n5  {'Address': 'dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n6                                                                           vpc-6f583517  \n7                                                                                      4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ClusterIdentifier</td>\n      <td>dwhcluster</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NodeType</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClusterStatus</td>\n      <td>deleting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MasterUsername</td>\n      <td>spark</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBName</td>\n      <td>sparkify</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Endpoint</td>\n      <td>{'Address': 'dwhcluster.cxovxmc95qdx.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>VpcId</td>\n      <td>vpc-6f583517</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NumberOfNodes</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'ResponseMetadata': {'RequestId': '205ac00e-78c3-44f6-843f-e73c2daa448d',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '205ac00e-78c3-44f6-843f-e73c2daa448d',\n   'content-type': 'text/xml',\n   'content-length': '200',\n   'date': 'Thu, 23 Apr 2020 10:28:27 GMT'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}