# Udactiy Data Warehouse In AWS
This project focuses on building a star schema from a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) and log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.
The project uses AWS and especially Redshift, an optimized postgres databased tuned for data warehousing capabilities.
The project provides the table modelling and etl pipeline to populate a Redshift cluster for analytics purposes for the fictitious company Sparkify 
## Installation
The whole project can be run through the infra.ipynb Jupyter notebook. That contains the following:
* Create IAM role for Redshift
* Create security group that will be attached
* Create Redshift cluster
* Create staging and analytics tabsles and insert data to them
* Run an analytics query
* Destroy the resources

## Configuration 
The config file dwh.cfg is epxected with the following structure
```
[AWS]
KEY=<YOUR_AWS_KEY>
SECRET=<YOUR_AWS_SECRET>
[CLUSTER]
HOST=<ENDPOINT_AS_RETURNED_FROM_STEP_3_2>
DWH_DB=<THE_DB_NAME>
DWH_DB_USER=<THE_DB_USER>
DWH_DB_PASSWORD=<THE_DB_PASSWORD>
DWH_PORT=<THE_DB_PORT>
DWH_DB_REGION=<THE_CLUSTER_REGION>
DWH_IAM_ROLE_NAME=<THE_NAME_OF_THE_ROLE_TO_ATTACH>
DWH_CLUSTER_TYPE=<CLUSTER_TYPE>
DWH_NUM_NODES=<NUM_OF_NODES>
DWH_NODE_TYPE=<NODE_TYPE>
DWH_CLUSTER_IDENTIFIER=<REDSHIFT_CLUSTER_NAME>
DWH_ROLE_ARN=<ARN_OF_THE_ROLE_TO_ATTACH>
[S3]
SONG_DATA=s3://udacity-dend/song_data
LOG_DATA=s3://udacity-dend/log_data
LOG_JSONPATH=s3://udacity-dend/log_json_path.json
[NETWORK]
ELASTIC_IP=<RESTRICT_TO_ELASTIC_IP>
```

## Purpose
This database that is being created helps the fictional company Sparkify create a fact and dimensions (star schema). That enables to:
* Supporting de-normalized data
* Enforcing referential integrity
* Reducing the time required to load large batches of data into a database 
* Improving query performance, i.e. queries generally run faster
* Enabling dimensional tables to be easily updated
* Enabling new ‘facts’ to be easily added i.e. regularly or selectively
* Simplicity i.e. navigating through the data is easy (analytical queries are easier)

## Database design
For the database the decision was made to create 5 tables with 1 fact table (songplays ) storing the fake events and 4 dimension tables 
There are also 2 staging tables that hold the s3 data ase the exists in the corresponding buckets
* songplays
* users
* artists
* songs
* time

* staging_events
* staging_songs

#### INSERTS

For the initial load of the staging_events I used Redshift's COPY command to get the relevant data from udacity's public song and log buckets
These tables where then used to insert data to the analytics tables

## ETL
The etl.py script runs the queries to first load the staging data and then insert the data in the analytics tables

## Future imrpovements 
* Since the type of queries that will run against our tables is not known, no hypothesis can be done for the distribution and sort keys
* Other IaC tools can be used to achieve setting the infrastructure required. One personal favorite is terraform which enables end to end handling of all aws resources
